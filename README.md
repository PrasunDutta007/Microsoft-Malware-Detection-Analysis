# Microsoft Malware Detection (Analysis)





## Software 

##### Jupyter Notebook &nbsp; <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/jupyter/jupyter-original.svg" height="20" width="20">



## Packages 

##### 1) Pandas &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 5) NLTK

##### 2) Scipy &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 6) Numpy
##### 3) Sci-Kit Learn &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 7) Plotly
##### 4) Seaborn &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 8) Matplotlib
 

  
## Installation of Packages

- Open cmd and type the following commands: 

```bash
  pip3 install pandas
```
```bash
  pip3 install matplotlib
```
```bash
  pip3 install nltk
```
```bash
  pip3 install numpy
```
```bash
  pip3 install scipy
```
```bash
  pip3 install scikit-learn
```
```bash
  pip3 install seaborn
```
```bash
  pip3 install plotly
```

## Concepts Used

- Hyperparameter Tuning
- K-Nearest Neighbours
- Logistic Regression
- Exploratory Data Analysis
- T-Distributed Stochastic Neighbourhood Embedding (t-SNE)
- Random Forest Classifier

## Problem Overview

Microsoft has been very active in building anti-malware products over the years and it runs it’s anti-malware utilities over 150 million computers around the world. 
This generates tens of millions of daily data points to be analyzed as potential malware. In order to be effective in analyzing and classifying such large amounts of data, we need to be able to group them into groups and identify their respective families.
This dataset provided by Microsoft contains about 9 classes of malware.</br>
<b>Source:</b> https://www.kaggle.com/c/malware-classification</br></br>

<b>The 9 Classes are as Follows:</b>
<ol>
  <li> Ramnit </li>
  <li> Lollipop </li>
  <li> Kelihos_ver3 </li>
  <li> Vundo </li>
  <li> Simda </li>
  <li> Tracur </li>
  <li> Kelihos_ver1 </li>
  <li> Obfuscator.ACY </li>
  <li> Gatak </li>
</ol>

There are nine different classes of malware that we need to classify based on the given data point => <b> Multi class classification problem </b>

The Performance of the entire model will be evaluated based on Two <b>Performance Metrices</b>:
##### 1) Multi class log-loss
##### 2) Confusion matrix


## Analysis

### Step 1: Separating out the .asm and .byte files
1) 150 GB of .asm files
2) 50 GB of byte files

### Step 2: Training and Testing Dataset

 Randomly Splitting the dataset into <ins>Training, Cross-Validation & Testing data</ins>

Let me now explain what exactly do I mean by <b>Train, Cross-Validation & Test</b>: 
1) The <b>Training Dataset</b> is used to Train the Model
2) The <b>Validation Dataset</b> is used to evaluate the given models among which one them is then chosen. This chosen model is then trained with the new Training Dataset.
3) Finally the the Trained Model is evaluated with the <b>Test Dataset</b><br><br>
In Steps 1 and 2, we do not want to evaluate the Candidate Models once. Instead, we prefer to Evaluate each model <b>Multiple Times</b> with different Dataset and take the <b>Average Score for our decision at Step 3</b>. 
If we have the luxury to vast amounts of data which we have in our case, this can be done easily. Otherwise, we can also use the trick of <b>K-fold</b> to resample the same dataset multiple times and pretend they are different. 
As we are evaluating the model, or hyperparameter, the model has to be trained from scratch, each time, without reusing the training result from previous attempts. We call this process <b>Cross Validation</b>

### Step 3: Exploratory Data Analysis

<b>Now What is Actually Meant By Exploratory Data Analysis ?</b><br><br>
Exploratory Data Analysis (EDA) is used by data scientists to analyze and investigate Data Sets and summarize their main characteristics, often employing data visualization methods. It helps determine how best to manipulate data sources to get the answers you need, making it easier for Data Scientists to discover <b>Patterns, Spot Anomalies, Test a Hypothesis, or Check Assumptions.</b>

EDA is primarily used to see what data can reveal beyond the <b>Formal Modeling or Hypothesis Testing Task</b> and provides a provides a better understanding of data set variables and the relationships between them. It can also help determine if the <b>Statistical Techniques</b> we are considering for data analysis are appropriate.


### Step 3.1: Analysing the Distribution of Malware Classes in Whole Data Set

<img src="Screenshots/Plot-1.png"/>

The above given <b>Bar Graph</b> illustrates the classes to which these malwares belong to and it can be observed clearly that the dataset contains very few data points which belong to 
<b>Class - 5</b> which will definitely act as a <b>Constraint</b> when building the future model predictions. Moreover, the data for each class are also not balanced, indicating it is an example
of <b>Imbalanced Classification</b>. 

### Step 3.2: Feature Extraction (Byte Files)

Here is a brief definition of what is meant by <b>Feature Extraction</b>: The Problem of selecting some Subset of a Learning Algorithm’s Input Variables upon 
which it should focus attention, while ignoring the rest. In other words, <b>Dimensionality Reduction!!</b><br>

<b>Mathematically Speaking,</b> Given a set of features <b>F = { f1 ,…, f2 ,…, fn }</b> the Feature Selection problem is to find a Subset that 
<b>“Maximizes the Learner’s Ability to Classify Patterns”</b>

### Step 3.2.1: File Size of Byte Files as a Feature

<b>Now a typical .byte file will have code like this:</b>
#### 00401000 56 8D 44 24 08 50 8B F1 E8 1C 1B 00 00 C7 06 08 
Here <b>00401100</b> represents the <b>Starting Address</b> of the Code & then we have a <b>Set of Two Hexadecimal Values</b> concatenated together

Before understanding how this code will be useful for our analysis, it is important to understand these two concepts namely:
#### 1) Unigrams
<img src="Screenshots/Unigram.png"/> </br>
To generate <b>1-grams or Unigrams</b> we pass the value of <b>n=1</b> in <b>N-grams</b> function of NLTK. But first, we split the sentence into tokens and 
then pass these tokens to ngrams function. These Unigrams are useful for creating capabilities like Autocorrect, Autocompletion of sentences, Text Summarization, Speech Recognition, etc.


#### 2) Bag of Words
Bag of words model helps convert the text into numerical representation (numerical feature vectors) such that the same can be used to train models using machine learning algorithms.<br>
Here are the key steps of fitting a bag-of-words model:<br><br>
a) Create a vocabulary indices of words or tokens from the entire set of documents. The vocabulary indices can be created in alphabetical order. <br>
b) Construct the numerical feature vector for each document that represents how frequent each word appears in different documents. 
The feature vector representing each will be sparse in nature as the words in each document will represent only a small subset of words out of all words (bag-of-words) present in entire set of documents.

#### Output:

<img src="Screenshots/text.png"/> <br>

The above Table is a sample output which keeps count of each of these <b>256 patterns</b> occuring in these byte files i.e, in <b>00 to FF</b> we have 
256 possibilities so it keeps track of these 256 possibilities. However, in our output we have 258 columns the two additional columns are for <b>Serial Number</b> and <b>File Id</b>

### Step 3.2.2: Multivariate Analysis

<b>Let's first understand what Multivariate Analysis even means ??</b><br><br>
Multivariate analysis deals with the statistical analysis of data collected on more than one dependent variable. Moreover, to be 
considered truly multivariate all the variables must be random and interrelated in such a way that their different effects can not meaningfully be interpreted separately.

The building block of the multivariate analysis is the variate. It is defined as the weighted sum of the variables, where the weights are defined by the multivariate techniques. 
The variate of n weighted variables(X1 to Xn) can be written as : <br>

<b>Variate</b> = X1*W1 + X2*W2 + X3*W3 + … + Xn*Wn <br>
where <b>X1, X2…Xn</b> are the <b>observed variables</b> and <br>
<b>W1, W2, W3…Wn</b> are the <b>weights.</b><br>

These variates capture the multivariate features of the analysis, thus in each technique, the variate acts as the focal point of the analysis.
For example, in multiple regression, the variate is determined in such a manner that the correlation between the dependent variable and the independent variables is maximum.<br><br>

Before moving towards our output it's important to first understand briefly what <b>t-SNE</b> means!!

<img src="Screenshots/Clusters.png" width="100" height="100"><img src="Screenshots/t-sne.gif"/></br>
In the Above Demonstration, the Picture in the Bottom Left corner represents the actual <b>Cluster Representation in the Higher-Dimensional Space</b>. Now t-SNE tries to plot this same structure
in a <b>reduced dimension</b>, like in the above case its being plotted in a <b>2-Dimensional space</b>.

So, basically t-SNE is a <b>non-linear dimensionality reduction algorithm</b> which finds <b>patterns</b> in the data based on the <b>similarity</b> of data points with features, the similarity of points is calculated as the <b>conditional probability</b> 
that a point A would choose point B as its neighbour. It then tries to <b>minimize</b> the difference between these conditional probabilities (or similarities) in higher-dimensional and lower-dimensional space 
for a perfect representation of data points in <b>lower-dimensional space</b>. 


Now, Let's understand the <b>Output</b> we have got after doing the T-SNE analysis:

<img src="Screenshots/Plot-3.png"/><br>

From the above Visual it's quite clear that the Malware byte code belonging to <b>Class 1,2 and 3</b> are quite nicely clustered and hence will give good results when we further run them in our Machine Learning Models. This is due to the fact that we have great number of data sets
based on these 3 classes. Some other classes too like <b>9 & 8</b> have small clusers formed though in a scattered manner. However, for the <b>rest of the classes</b> we observe that it is pretty much scattered throughout
without much observable pattern.

### Step 4: Train Test Split

We split our Byte files in a random ratio between Train, Test & Cross-Validation. This step is in continuation to Step 2 where we now actually implement it, before implementing the above
dataset into Machine Learning Models.

### Step 5: Machine Learning Models (Only on Byte Files)

A <b>“Model”</b> in Machine Learning is the <b>output</b> of a <b>Machine Learning Algorithm</b> run on data.
A model represents what was <b>learned</b> by a machine learning algorithm.
The model is the “thing” that is <b>saved</b> after running a machine learning algorithm on <b>training data</b> and represents the rules, numbers, and any other algorithm-specific data structures required to make <b>predictions.</b>

Before delving deep into each of the Models, let us first understand what <b>Multiclass Log-Loss</b> means:<br>

Log Loss is one of the most important classification metric based on probabilities.
It's hard to interpret raw log-loss values, but log-loss is still a <b>good metric for comparing models</b>. For any given problem, a <b>lower log-loss</b> value means <b>better predictions</b>.
Log Loss is a slight twist on something called as the <b>Likelihood Function</b>. So, we will start by understanding the likelihood function.
The likelihood function answers the question <b>"How likely did the model think the actually observed set of outcomes was."</b></br></br>
Lets have a look at the <b>Formula</b> for <b>Multiclass Log-Loss</b>:

<img src="Screenshots/Formula-1.png"/><br>

Here, where <b>N</b> is the number of <b>samples or instances</b>,<br>
<b>M</b> is the number of <b>possible labels</b>,<br> 
<b>y<sub>ij</sub></b> is a <b>binary indicator</b> of whether or not label j is the correct classification for instance i, and <br>
<b>p<sub>ij</sub></b> is the <b>model probability</b> of assigning label j to instance i.<br> 
A <b>perfect classifier</b> would have a Log Loss of <b>precisely zero</b>. <b>Less ideal classifiers</b> have progressively <b>larger values of Log Loss.</b>

### Step 5.1: Random Model

A Random Model in Machine Learning in our case, will compute the <b>probability values</b> of all the classes on a Random basis and will give us a <b>Cross-Vaidation, Test and Misclassified points</b> value.
Note that these values have been generated in a Random manner and not based on any Algorithms or Patterns. So, if we make any Future Models we need to make sure that those model's <b>log-loss</b>
<b>values are less than the random model's value</b> cause if it exceeds the random models's value then it is an indication that something is seriously going wrong in the new model.
So, in the Newer Models we will try to keep our <b>log-loss values as close to 0 as possible</b>. Since, a <b>log-loss of 0</b> indicates an almost perfect model.


## References & Resources

- https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/
- https://medium.com/@mehulved1503/feature-selection-and-feature-extraction-in-machine-learning-an-overview-57891c595e96
- https://machinelearningknowledge.ai/generating-unigram-bigram-trigram-and-ngrams-in-nltk/
- https://vitalflux.com/text-classification-bag-of-words-model-python-sklearn/
- https://www.ibm.com/cloud/learn/exploratory-data-analysis
- https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/
- https://towardsdatascience.com/introduction-to-data-analysis-basic-concepts-involved-in-multivariate-analysis-4295cc125052
- https://distill.pub/2016/misread-tsne/
- https://www.geeksforgeeks.org/ml-t-distributed-stochastic-neighbor-embedding-t-sne-algorithm/
- https://www.section.io/engineering-education/introduction-to-random-forest-in-machine-learning/
- https://www.kaggle.com/dansbecker/what-is-log-loss
- https://www.cs.princeton.edu/courses/archive/spring16/cos495/slides/ML_basics_lecture7_multiclass.pdf
- https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a
